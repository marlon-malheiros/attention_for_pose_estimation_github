{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar se rodar em alguma cloud\n",
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca padrão\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from importlib import reload  # (Descomente se necessário)\n",
    "import random\n",
    "\n",
    "# Bibliotecas de terceiros\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# PyTorch e torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Get the parent directory of the current notebook file\n",
    "from pathlib import Path\n",
    "current_notebook_path = Path().resolve()  # Resolve the current working directory\n",
    "parent_folder = current_notebook_path.parent\n",
    "\n",
    "# Append the parent folder to sys.path\n",
    "sys.path.append(str(parent_folder))\n",
    "import minhas_funcoes\n",
    "\n",
    "# Kaggle\n",
    "# (Descomente se necessário)\n",
    "# sys.path.append(r'/kaggle/input/custom_functions/pytorch/default/2')\n",
    "# import minhas_funcoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Carregar detecções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "# path_deteccoes = r'/kaggle/input/deteccoes-finais/deteccoes_finais.json'\n",
    "# Local\n",
    "path_deteccoes = r'..\\detector\\deteccoes_finais.json'\n",
    "\n",
    "with open(path_deteccoes, \"r\") as f:\n",
    "    detecoes_transformadas_com_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Filtrar detecções válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Valid/Invalid samples: 15562/106\n",
      "15562\n"
     ]
    }
   ],
   "source": [
    "# Teste local com subset pequeno\n",
    "# ann_file = r'c:\\Users\\Marlon\\Downloads\\annotations\\person_keypoints_val2017.json'\n",
    "# path_imgs = r'C:\\Users\\Marlon\\Downloads\\val2017'\n",
    "\n",
    "# Kaggle\n",
    "# ann_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json'\n",
    "# path_imgs = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "\n",
    "# Treinamento local\n",
    "ann_file = r\"c:\\Users\\Marlon\\Downloads\\annotations\\person_keypoints_train2017.json\"\n",
    "path_imgs = r\"c:\\Users\\Marlon\\Downloads\\train2017\"\n",
    "\n",
    "annotations_file = ann_file\n",
    "\n",
    "img_ids_bboxes = detecoes_transformadas_com_ids\n",
    "valid_samples, invalid_samples = minhas_funcoes.filter_images_by_keypoints(img_ids_bboxes, annotations_file)\n",
    "print(f\"Valid/Invalid samples: {len(valid_samples)}/{len(invalid_samples)}\")\n",
    "print(len(valid_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOPoseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, valid_samples, target_size=(256, 256), heatmap_size=(64, 64), transform=None):\n",
    "        \"\"\"\n",
    "        Custom COCO pose dataset for keypoint heatmap generation.\n",
    "        \n",
    "        Args:\n",
    "            image_dir (str): Directory containing the images.\n",
    "            annotations_file (str): Path to the COCO annotations JSON file.\n",
    "            valid_samples (list): List of tuples (image_id, bbox) for images with valid bboxes.\n",
    "            target_size (tuple): Desired image size after resizing and padding (h, w).\n",
    "            heatmap_size (tuple): Desired heatmap size for each keypoint (h, w).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotations_file)\n",
    "        self.valid_samples = valid_samples\n",
    "        self.target_size = target_size  # (h, w)\n",
    "        self.heatmap_size = heatmap_size  # (h, w)\n",
    "        self.keypoint_names = [\n",
    "            \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\", \"Left Shoulder\",\n",
    "            \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\", \"Left Wrist\", \"Right Wrist\",\n",
    "            \"Left Hip\", \"Right Hip\", \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\"\n",
    "        ]\n",
    "        # Change here to load the transforms\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def create_gaussian_heatmap(self, height, width, center_x, center_y, sigma=2):\n",
    "        \"\"\"Generates a 2D Gaussian heatmap centered at (center_x, center_y).\"\"\"\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        d2 = (x_grid - center_x) ** 2 + (y_grid - center_y) ** 2\n",
    "        gaussian = np.exp(-d2 / (2 * sigma ** 2))\n",
    "        return gaussian\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get the bounding box and image_id from valid_samples\n",
    "        image_id, bbox = self.valid_samples[idx]\n",
    "\n",
    "        # 2. Load image and retrieve annotation\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_name = image_info['file_name']\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_width, image_height = image.size  # (w, h)\n",
    "\n",
    "        # Adjust the bounding box if it exceeds the image boundaries\n",
    "        x_min, y_min, width, height = bbox\n",
    "        x_max = min(image_width, x_min + width)\n",
    "        y_max = min(image_height, y_min + height)\n",
    "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "        width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "        # Crop the image using the adjusted bounding box\n",
    "        image_cropped = image.crop((x_min, y_min, x_min + width, y_min + height))\n",
    "\n",
    "        # Resize the cropped image to the target size\n",
    "        aspect_ratio = image_cropped.height / image_cropped.width\n",
    "        if aspect_ratio > 1:\n",
    "            new_height = self.target_size[0]\n",
    "            new_width = int(self.target_size[1] / aspect_ratio)\n",
    "        else:\n",
    "            new_width = self.target_size[1]\n",
    "            new_height = int(self.target_size[0] * aspect_ratio)\n",
    "\n",
    "        image_resized = image_cropped.resize((new_width, new_height), Image.LANCZOS)\n",
    "        delta_h, delta_w = self.target_size[0] - new_height, self.target_size[1] - new_width\n",
    "        padding = (delta_w // 2, delta_h // 2, delta_w - delta_w // 2, delta_h - delta_h // 2)\n",
    "        image_padded = ImageOps.expand(image_resized, padding, fill=(0, 0, 0))\n",
    "\n",
    "        # Load and adjust keypoints\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=[self.coco.getCatIds(catNms=['person'])[0]], iscrowd=False)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        keypoints = annotations[0]['keypoints']\n",
    "        keypoints = [(keypoints[i], keypoints[i + 1], keypoints[i + 2]) for i in range(0, len(keypoints), 3)]\n",
    "        adjusted_keypoints = [(x - x_min, y - y_min, v) for x, y, v in keypoints]\n",
    "        scale_y, scale_x = new_height / height, new_width / width\n",
    "        scaled_keypoints = [(x * scale_x, y * scale_y, v) for x, y, v in adjusted_keypoints]\n",
    "        adjusted_keypoints_padded = [(x + padding[0], y + padding[1], v) for x, y, v in scaled_keypoints]\n",
    "\n",
    "        # Generate heatmaps for each keypoint\n",
    "        scale_y_hm = self.heatmap_size[0] / self.target_size[0]\n",
    "        scale_x_hm = self.heatmap_size[1] / self.target_size[1]\n",
    "        heatmaps = []\n",
    "        for x, y, v in adjusted_keypoints_padded:\n",
    "            if v == 2:\n",
    "                x_hm, y_hm = int(x * scale_x_hm), int(y * scale_y_hm)\n",
    "                heatmap = self.create_gaussian_heatmap(self.heatmap_size[0], self.heatmap_size[1], x_hm, y_hm, sigma=2)\n",
    "            else:\n",
    "                # Also i think i will change this behaviour\n",
    "                # It can be better to create a heatmap with the peak at (0,0), just like in the dataset right?\n",
    "                heatmap = np.zeros(self.heatmap_size)\n",
    "                \n",
    "            heatmaps.append(torch.tensor(heatmap))\n",
    "\n",
    "        # Convert final image to tensor and return\n",
    "        image_tensor = torch.tensor(np.array(image_padded)).permute(2, 0, 1).float() / 255.0\n",
    "        # heatmaps_tensor = torch.stack(heatmaps)\n",
    "        heatmaps_tensor = torch.stack(heatmaps).float()  # Ensure float dtype for heatmaps\n",
    "\n",
    "        # If you have a transform for image+heatmaps:\n",
    "        if self.transform is not None:\n",
    "            # image_tensor should be a float tensor and heatmaps_tensor a float tensor before transform\n",
    "            # Make sure both are on CPU and float type\n",
    "            # image_tensor = image_tensor.to(torch.float)\n",
    "            # heatmaps_tensor = heatmaps_tensor.to(torch.float)\n",
    "            image_tensor, heatmaps_tensor = self.transform(image_tensor, heatmaps_tensor)\n",
    "\n",
    "        return image_tensor, heatmaps_tensor, adjusted_keypoints_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Classe para sincronizar augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPairTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image_tensor, heatmaps_tensor):\n",
    "        # Ensure image_tensor is float in range [0, 1]\n",
    "        assert image_tensor.dtype == torch.float, \"Image tensor must be of dtype torch.float\"\n",
    "        assert image_tensor.max() <= 1.0, \"Image tensor values must be in range [0, 1]\"\n",
    "\n",
    "        # Horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            image_tensor = TF.hflip(image_tensor)\n",
    "            heatmaps_tensor = torch.flip(heatmaps_tensor, dims=[2])  # Flip along width (W)\n",
    "\n",
    "        # Random rotation\n",
    "        angle = random.uniform(-15, 15)\n",
    "        image_tensor = TF.rotate(image_tensor, angle)  # Rotate image tensor\n",
    "        heatmaps_tensor = TF.rotate(heatmaps_tensor.unsqueeze(1), angle).squeeze(1)  # Rotate heatmaps\n",
    "\n",
    "        # Add more transformations as needed\n",
    "\n",
    "        return image_tensor, heatmaps_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Inicializar modelo de estimação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 + Self-Attention + Conv-Deconv\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet50 and remove the last two layers (FC and AvgPool)\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "model = models.resnet50(weights=weights)\n",
    "backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "class ConvSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, height, width):\n",
    "        super(ConvSelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Learnable positional encodings\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, out_channels, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # Add positional encoding to the input\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Flatten spatial dimensions for Q, K, V\n",
    "        q = self.query(x).flatten(2)  # Shape: [B, C, H*W]\n",
    "        k = self.key(x).flatten(2)  # Shape: [B, C, H*W]\n",
    "        v = self.value(x).flatten(2)  # Shape: [B, C, H*W]\n",
    "\n",
    "        # Transpose Q and K for batch matrix multiplication\n",
    "        q = q.permute(0, 2, 1)  # Shape: [B, H*W, C]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.bmm(q, k)  # Shape: [B, H*W, H*W]\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "\n",
    "        # Apply attention\n",
    "        out = torch.bmm(v, attn_weights.transpose(1, 2))  # Shape: [B, C, H*W]\n",
    "        out = out.view(b, c, h, w)  # Reshape back to [B, C, H, W]\n",
    "\n",
    "        return out + x  # Residual connection\n",
    "\n",
    "\n",
    "# PoseEstimationHead (Conv-Deconv layers)\n",
    "class PoseEstimationHead(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super(PoseEstimationHead, self).__init__()\n",
    "        # Reduce channels from 2048 to 256\n",
    "        self.conv1 = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Deconvolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Final conv layer to get keypoint heatmaps\n",
    "        self.final_conv = nn.Conv2d(256, num_keypoints, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        x = self.relu(self.bn1(self.deconv1(x)))\n",
    "        x = self.relu(self.bn2(self.deconv2(x)))\n",
    "        x = self.relu(self.bn3(self.deconv3(x)))\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PoseEstimationModel(nn.Module):\n",
    "    def __init__(self, num_keypoints=17, embed_dim=2048, height=7, width=7):\n",
    "        super(PoseEstimationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.attention = ConvSelfAttention(embed_dim, embed_dim, height, width)\n",
    "        self.head = PoseEstimationHead(num_keypoints)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # Extract features using ResNet50 backbone\n",
    "        x = self.attention(x)  # Apply self-attention\n",
    "        x = self.head(x)  # Generate keypoint heatmaps\n",
    "        return x\n",
    "    \n",
    "# Initialize the modified model\n",
    "model = PoseEstimationModel()\n",
    "\n",
    "# Print model summary\n",
    "print(\"ResNet50 + CSA + Positional Encoding + Conv-Deconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Move the Model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Setup data augment e data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.91s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "size_input = (224, 224)\n",
    "size_heatmap = (56, 56)\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = MyPairTransform()\n",
    "\n",
    "# Update the dataset class to apply transformations\n",
    "dataset = COCOPoseDataset(path_imgs, annotations_file, valid_samples, size_input, size_heatmap, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "val_split = 0.1\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(val_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "# batch_size = 100\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marlon\\OneDrive\\Área de Trabalho\\attention_for_pose_estimation_github\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assuming `model` is already defined; replace with your model initialization if not\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=80, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Loop de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Bacthes:  68%|██████▊   | 480/701 [04:31<02:05,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 48\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Após o loop de treinamento, calcular a média da loss\u001b[39;00m\n\u001b[0;32m     51\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store losses for plotting later\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Initialize a list to store learning rates\n",
    "learning_rate_log = []\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 300\n",
    "patience = 30\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "patience_counter = 0  # Initialize the patience counter\n",
    "\n",
    "# Minimum number of epochs\n",
    "min_epochs = 50\n",
    "max_epochs = 300\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Track epoch start time\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, heatmaps, _ in train_loader:\n",
    "        inputs, heatmaps = inputs.float().to(device), heatmaps.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Masking the validation loss for visible points\n",
    "        mask = (heatmaps.sum(dim=(2, 3)) > 0).float().unsqueeze(-1).unsqueeze(-1)  # Shape: [batch_size, num_keypoints, 1, 1]\n",
    "        visible_outputs = outputs * mask  # Apply mask to outputs\n",
    "        visible_heatmaps = heatmaps * mask  # Apply mask to ground truth heatmaps\n",
    "\n",
    "        # Compute masked loss\n",
    "        loss = criterion(visible_outputs, visible_heatmaps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # After the training loop, calculate the average loss\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Training Loss: {epoch_train_loss:.8f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_heatmaps, _ in val_loader:\n",
    "            val_inputs, val_heatmaps = val_inputs.float().to(device), val_heatmaps.float().to(device)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "\n",
    "            val_mask = (val_heatmaps.sum(dim=(2, 3)) > 0).float().unsqueeze(-1).unsqueeze(-1)            \n",
    "            visible_val_outputs = val_outputs * val_mask\n",
    "            visible_val_heatmaps = val_heatmaps * val_mask\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss += criterion(visible_val_outputs, visible_val_heatmaps).item() * val_inputs.size(0)\n",
    "    \n",
    "    # Validation loss   \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {epoch_val_loss:.8f}\")\n",
    "    \n",
    "    # Epoch end time\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} took {epoch_duration:.2f} seconds.\")\n",
    "\n",
    "    # Record the learning rate after each epoch\n",
    "    current_lr = optimizer.param_groups[0]['lr']  # Assuming a single parameter group\n",
    "    learning_rate_log.append(current_lr)\n",
    "    print(f\"Epoch {epoch+1}, Learning Rate: {current_lr}\")\n",
    "    \n",
    "    # Check for Early Stopping and Save Best Model Checkpoint\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        save_path = f\"best_model.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': epoch_train_loss,\n",
    "            'val_loss': epoch_val_loss\n",
    "        }, save_path)  # Save the best model checkpoint\n",
    "        print(\"Model improved. Saving the best model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience and epoch >= min_epochs:\n",
    "        print(f\"Min number epochs reached, with no improvement in validation loss. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    # If gets to max number of epochs\n",
    "    if epoch == max_epochs:\n",
    "        print(f\"Reached maximum number of epochs ({max_epochs}). Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "# Save the learning rate log to a file (optional)\n",
    "with open(\"learning_rate_log.txt\", \"w\") as f:\n",
    "    for lr in learning_rate_log:\n",
    "        f.write(f\"{lr}\\n\")\n",
    "\n",
    "with open(\"train_losses.txt\", \"w\") as f:\n",
    "    for value in train_losses:\n",
    "        f.write(f\"{value}\\n\")\n",
    "\n",
    "with open(\"val_losses.txt\", \"w\") as f:\n",
    "    for value in val_losses:\n",
    "        f.write(f\"{value}\\n\")\n",
    "\n",
    "print(f\"End training loop after {epoch + 1} epochs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
