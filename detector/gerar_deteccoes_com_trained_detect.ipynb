{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesse arquivo é feita a detecção de todas as imagens de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marlon\\AppData\\Local\\Temp\\ipykernel_7504\\2893092414.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(r\"best_model_faster_rcnn_25_epocas_treinadas_best_at_22.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeiro o modelo é inicializado\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# Passa o modelo p/ GPU se disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model is on {device}\")\n",
    "\n",
    "# Carrega os pesos\n",
    "checkpoint = torch.load(r\"best_model_faster_rcnn_25_epocas_treinadas_best_at_22.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Class\n",
    "Usada para passar cada img pelo detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonDetectionDataset(Dataset):\n",
    "    def __init__(self, img_ids, annotations_file, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_ids (list): List of filtered image IDs (img_filtradas).\n",
    "            annotations_file (str): Path to the COCO annotations JSON file.\n",
    "            image_dir (str): Path to the directory containing images.\n",
    "            transform (callable, optional): Optional transform to apply to each image.\n",
    "        \"\"\"\n",
    "        self.img_ids = img_ids\n",
    "        self.coco = COCO(annotations_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Preload image metadata for each image ID\n",
    "        self.image_metadata = {\n",
    "            image_id: self.coco.loadImgs(image_id)[0] for image_id in self.img_ids\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image ID and associated metadata\n",
    "        image_id = self.img_ids[idx]\n",
    "        img_info = self.image_metadata[image_id]\n",
    "        image_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get annotations for the image ID\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=[self.coco.getCatIds(catNms=['person'])[0]], iscrowd=False)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, width, height = ann['bbox']\n",
    "            boxes.append([x, y, x + width, y + height])  # Convert to [x_min, y_min, x_max, y_max]\n",
    "            labels.append(1)  # Label \"1\" for \"person\"\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary with image metadata (e.g., width, height, and filename)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"width\": img_info[\"width\"],\n",
    "            \"height\": img_info[\"height\"],\n",
    "            \"file_name\": img_info[\"file_name\"]  # Adding the file name here\n",
    "        }\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Funções de filtragem\n",
    "Mesmas dos arquivos do estimador, só uso o detector nas imagens filtradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Total de imagens que possuem uma única pessoa: 1045\n",
      "N. de imgs após remoção das vazias: 921\n",
      "Número final de imagens com n mínimo 7 pontos: 721\n"
     ]
    }
   ],
   "source": [
    "def filtrar_imagens_com_somente_uma_pessoa(coco):\n",
    "    \"\"\"\n",
    "    Filtra imagens que contêm somente uma pessoa usando a API COCO.\n",
    "\n",
    "    Parameters:\n",
    "    coco (COCO): Objeto COCO com as anotações carregadas.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de IDs das imagens que contêm somente uma pessoa.\n",
    "    \"\"\"\n",
    "    image_ids = coco.getImgIds()\n",
    "    image_with_one_person_only = []\n",
    "\n",
    "    for image_id in image_ids:\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id, catIds=[1], iscrowd=False)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "        if len(annotations) == 1:  # Somente uma pessoa na imagem\n",
    "            image_with_one_person_only.append(image_id)\n",
    "\n",
    "    print(f\"Total de imagens que possuem uma única pessoa: {len(image_with_one_person_only)}\")\n",
    "    return image_with_one_person_only\n",
    "\n",
    "\n",
    "def filtrar_imgs_com_zeros(coco, imgs_1_person):\n",
    "    \"\"\"\n",
    "    Filtra imagens que possuem todos os keypoints iguais a zero.\n",
    "\n",
    "    Parameters:\n",
    "    coco (COCO): Objeto COCO com as anotações carregadas.\n",
    "    imgs_1_person (list): Lista de IDs das imagens que contêm somente uma pessoa.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de IDs das imagens que contêm keypoints válidos.\n",
    "    \"\"\"\n",
    "    all_zeros_ids = []\n",
    "\n",
    "    for image_id in imgs_1_person:\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id, catIds=[1], iscrowd=False)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "        keypoints = annotations[0]['keypoints']\n",
    "        all_zeros = all(k == 0 for k in keypoints)\n",
    "        if all_zeros:\n",
    "            all_zeros_ids.append(image_id)\n",
    "\n",
    "    filtered_ids = set(imgs_1_person) - set(all_zeros_ids)\n",
    "    filtered_ids = list(filtered_ids)\n",
    "\n",
    "    print(f'N. de imgs após remoção das vazias: {len(filtered_ids)}')\n",
    "\n",
    "    return filtered_ids\n",
    "\n",
    "def filtrar_imgs_k_pontos(imgs, k):\n",
    "    \"\"\"\n",
    "    Filtra imagens com no mínimo k pontos vísiveis.\n",
    "    \"\"\"\n",
    "    img_filtradas = []\n",
    "\n",
    "    for img_id in imgs:        \n",
    "        # Get annotations\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=[1], iscrowd=False)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Get keypoints\n",
    "        keypoints_raw = annotations[0]['keypoints']\n",
    "        keypoints = []\n",
    "        for i in range(0, len(keypoints_raw), 3):\n",
    "            keypoints.append([keypoints_raw[i], keypoints_raw[i+1], keypoints_raw[i+2]])\n",
    "        \n",
    "        # Count number of visible points\n",
    "        n_visible_keypoints = 0\n",
    "        for point_set in keypoints:\n",
    "            if point_set[2] == 2:\n",
    "                n_visible_keypoints += 1\n",
    "        \n",
    "        if n_visible_keypoints >= k:\n",
    "            img_filtradas.append(img_id)\n",
    "        \n",
    "    return img_filtradas\n",
    "\n",
    "def get_paths_from_ids(img_ids, coco, path_imgs):\n",
    "    img_paths = []\n",
    "    for img_id in img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(path_imgs, img_info['file_name'])\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "    return img_paths\n",
    "\n",
    "# Treino local completo\n",
    "# ann_file = r\"C:\\Users\\Marlon\\Downloads\\COCO_Dataset\\annotations\\person_keypoints_train2017.json\"\n",
    "# coco = COCO(ann_file)\n",
    "# path_imgs = r\"C:\\Users\\Marlon\\Downloads\\COCO_Dataset\\train2017\"\n",
    "\n",
    "# Kaggle\n",
    "# ann_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json'\n",
    "# coco = COCO(ann_file)\n",
    "# path_imgs = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "\n",
    "# Teste local menor\n",
    "ann_file = r\"C:\\Users\\Marlon\\Downloads\\COCO_Dataset\\annotations\\person_keypoints_val2017.json\"\n",
    "coco = COCO(ann_file)\n",
    "path_imgs = r\"C:\\Users\\Marlon\\Downloads\\COCO_Dataset\\val2017\"\n",
    "\n",
    "filter_images_1p = filtrar_imagens_com_somente_uma_pessoa(coco)\n",
    "filter_no_zero_imgs = filtrar_imgs_com_zeros(coco, filter_images_1p)\n",
    "img_filtradas = filter_no_zero_imgs\n",
    "\n",
    "# Número mínimo de pontos visíveis\n",
    "k = 7\n",
    "img_filtradas = filtrar_imgs_k_pontos(img_filtradas, k)\n",
    "print(f\"Número final de imagens com n mínimo {k} pontos: {len(img_filtradas)}\")\n",
    "\n",
    "path_img_filtradas = get_paths_from_ids(img_filtradas, coco, path_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[393226, 532493, 45070, 401446, 213033, 471087, 475191, 561223, 356427, 32861, 192607, 442463, 311394, 561256, 221291, 49259, 94326, 417911, 118921, 270474, 356505, 266409, 90284, 188592, 229553, 356531, 213171, 549055, 110784, 389316, 65736, 344268, 16598, 516318, 295138, 33005, 545007, 458992, 135410, 426241, 213255, 250127, 426268, 4395, 205105, 61747, 463174, 336209, 414034, 483667, 8532, 270677, 393569, 434548, 442746, 426376, 336265, 369037, 459153, 102805, 94614, 110999, 389532, 254368, 106912, 102820, 381360, 135604, 262587, 278973, 238013, 192964, 340451, 451043, 205289, 258541, 455157, 352760, 29187, 61960, 25096, 451084, 270883, 365095, 127530, 389684, 295478, 152120, 270908, 324158, 373315, 172617, 451155, 107094, 123480, 33368, 438876, 565853, 504415, 172649, 406129, 275058, 569972, 565877, 311928, 520832, 193162, 25228, 328337, 184978, 152214, 4765, 369310, 246436, 127660, 414385, 389812, 402118, 565962, 471756, 520910, 99024, 479953, 74457, 488166, 262895, 250619, 492284, 217872, 785, 82715, 353051, 426795, 160556, 553776, 17207, 553788, 90956, 172877, 25424, 426836, 209753, 574297, 557916, 525155, 33638, 451435, 177015, 37751, 13177, 467848, 410510, 13201, 54164, 193429, 328601, 263068, 398237, 340894, 222118, 99242, 463802, 275392, 5060, 308165, 144333, 173008, 476119, 512985, 394199, 33759, 46048, 422886, 320490, 140270, 578545, 279541, 242678, 553990, 324614, 168974, 164883, 259097, 197658, 17436, 361506, 443426, 476215, 181303, 382009, 574520, 427077, 365642, 476258, 21604, 390246, 119911, 226408, 74860, 128112, 42102, 103548, 193674, 136334, 50326, 337055, 38048, 345252, 308394, 554156, 193717, 79031, 246968, 115898, 320696, 423104, 87244, 177357, 492758, 439522, 62692, 439525, 38118, 9448, 312552, 578792, 263403, 197870, 455937, 378116, 148739, 161032, 9483, 394510, 464144, 202001, 296222, 521509, 288042, 578871, 394559, 308545, 66886, 402774, 529762, 357742, 210299, 79229, 54654, 378244, 537991, 312720, 562581, 5529, 349594, 230819, 492968, 443844, 50638, 13774, 1490, 570834, 407002, 439773, 99810, 17899, 17905, 202228, 513524, 357888, 329219, 529939, 468505, 124442, 460333, 9772, 190007, 398905, 292415, 230983, 431693, 83540, 378454, 276055, 144984, 464476, 185950, 132703, 374369, 468577, 554595, 575081, 67180, 484978, 50811, 165518, 128654, 54931, 34452, 485027, 480936, 112298, 390826, 341681, 63154, 157365, 186042, 190140, 394940, 321214, 214720, 497344, 370375, 485071, 575187, 116439, 567011, 521956, 444142, 173830, 259854, 18193, 30504, 456496, 350002, 358195, 386879, 350019, 341828, 198489, 386912, 22371, 190307, 448365, 198510, 456559, 489339, 571264, 493442, 329614, 14226, 546717, 341921, 194471, 231339, 239537, 333745, 464824, 92091, 22479, 108495, 223188, 456662, 210915, 391140, 104424, 161781, 317433, 415748, 370711, 362520, 284698, 473118, 473121, 129062, 149568, 161861, 579655, 469067, 563281, 161875, 161879, 18519, 284764, 407646, 407650, 329827, 481386, 30828, 403565, 313454, 51314, 170099, 366711, 297084, 358525, 411774, 481413, 415882, 489611, 510095, 563349, 80022, 227482, 440475, 546976, 227491, 182441, 22705, 71877, 153797, 280779, 493772, 325838, 170191, 391375, 362716, 55528, 465129, 96493, 313588, 321790, 186624, 145665, 235784, 149770, 563470, 198928, 80153, 465179, 481567, 190756, 481573, 366884, 489764, 133418, 551215, 383289, 235836, 6460, 473406, 579902, 125257, 280930, 325991, 498032, 457078, 285047, 190841, 244099, 530820, 297353, 84362, 199055, 80273, 399764, 51610, 88485, 428454, 559543, 133567, 489924, 281032, 289229, 117197, 485844, 2532, 170474, 326128, 31217, 477689, 51712, 547336, 10764, 551439, 158227, 182805, 125472, 27186, 576052, 408120, 39480, 96825, 289343, 424521, 166478, 248400, 334417, 23126, 72281, 531036, 252507, 162415, 383606, 367228, 412286, 39551, 395903, 199310, 55950, 400044, 527029, 334530, 375493, 47819, 101068, 223955, 223959, 129756, 273132, 154358, 121591, 109313, 76547, 477955, 473869, 244496, 199442, 162581, 404249, 80671, 506656, 449312, 273198, 191288, 154425, 289594, 23359, 314177, 269121, 469828, 281414, 121673, 371529, 183127, 346968, 187243, 551794, 203639, 88951, 564091, 551804, 199551, 88970, 314264, 179112, 7088, 474039, 355257, 342971, 60347, 261061, 441286, 170955, 64462, 515025, 437205, 117719, 187362, 19432, 474095, 117744, 89078, 97278, 404484, 273420, 490515, 68628, 158744, 347174, 449579, 576566, 330818, 527427, 449603, 437331, 3156, 40036, 437351, 7278, 564336, 498807, 388215, 449661, 556158, 347265, 384136, 175251, 289960, 474293, 191672, 122046, 187585, 212166, 261318, 232649, 367818, 64718, 396518, 158956, 15597, 93437, 359677, 527616, 212226, 195842, 306437, 568584, 425226, 515350, 52507, 142620, 245026, 15660, 535858, 494913, 269632, 27972, 404805, 281929, 32081, 314709, 445792, 273760, 363875, 64868, 52591, 183675, 322944, 527750, 376206, 425361, 400794, 343453, 261535, 109992, 220584, 400815, 482735, 454067, 48564, 73153, 216516, 581062, 257478, 343496, 19924, 60899, 343524, 286182, 458223, 151051, 306700, 237071, 458255, 130579, 130586, 130599, 376365, 265777, 237118, 364102, 351823, 507473, 491090, 15956, 368212, 187990, 265816, 257624, 532058, 364126, 261732, 409198, 368294, 466602, 81594, 544444, 581317, 413395, 425702, 474854, 560880, 470773, 229111, 343803, 253695, 261888, 454404, 85772, 171788, 560911, 356125, 380711, 384808, 274219, 122672, 540466, 40757, 565045, 32570, 433980, 139077, 257865, 573258, 331604, 401244, 393056, 401250, 163682, 257896, 20333, 339823, 253819, 343937, 552842, 499622, 180135, 327592, 327601, 102331, 327617, 442306, 491464, 503755, 102356, 397279, 94185, 274411, 24567, 450559]\n"
     ]
    }
   ],
   "source": [
    "print(img_filtradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = ToTensor()\n",
    "# Create the dataset\n",
    "dataset = PersonDetectionDataset(img_ids=img_filtradas, annotations_file=ann_file, image_dir=path_imgs, transform=transform)\n",
    "\n",
    "# Create the dataloader\n",
    "# data_loader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,  # Increase if GPU memory allows\n",
    "    shuffle=False,\n",
    "    pin_memory=True,  # Disable pinned memory for now  \n",
    "    collate_fn=lambda x: tuple(zip(*x))  # Necessary for variable-sized detection data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Função auxiliar: Merge bbox\n",
    "\n",
    "Algumas detecções são duplicadas, uma bounding box está dentro da outra. E na grande maioria das vezes existe uma boa sobreposição entre as bbox, por isso estou só as fundindo. Avaliando o resultadoo eles pareceram satisfatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_merge_boxes(detections, score_threshold=0.5, containment_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Custom function to merge bounding boxes if one is mostly contained within another.\n",
    "    \"\"\"\n",
    "    device = detections['boxes'].device  # Ensure device compatibility\n",
    "\n",
    "    # Filter for \"person\" boxes above the score threshold\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for i in range(len(detections['labels'])):\n",
    "        if detections['labels'][i] == 1 and detections['scores'][i] > score_threshold:\n",
    "            boxes.append(detections['boxes'][i])\n",
    "            scores.append(detections['scores'][i])\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return {\"boxes\": [], \"scores\": []}  # No boxes to keep\n",
    "\n",
    "    boxes = torch.stack(boxes).to(device)\n",
    "    scores = torch.tensor(scores, device=device)\n",
    "\n",
    "    # Initialize merged boxes and scores\n",
    "    merged_boxes = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in used:\n",
    "            continue\n",
    "\n",
    "        x_min, y_min, x_max, y_max = boxes[i]\n",
    "        score = scores[i]\n",
    "        \n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            if j in used:\n",
    "                continue\n",
    "            \n",
    "            # Calculate IoU to check overlap\n",
    "            iou = calculate_iou(boxes[i], boxes[j])\n",
    "\n",
    "            # If IoU is high (boxes significantly overlap), merge them\n",
    "            if iou > containment_threshold or is_contained(boxes[i], boxes[j], containment_threshold):\n",
    "                # Expand the outer box to include both\n",
    "                x_min = min(x_min, boxes[j][0])\n",
    "                y_min = min(y_min, boxes[j][1])\n",
    "                x_max = max(x_max, boxes[j][2])\n",
    "                y_max = max(y_max, boxes[j][3])\n",
    "                used.add(j)  # Mark box j as used\n",
    "\n",
    "        # Append the merged (or standalone) box\n",
    "        merged_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    # Convert merged_boxes list to tensor\n",
    "    merged_boxes = torch.tensor(merged_boxes, dtype=torch.float32)\n",
    "\n",
    "    # Return merged boxes and corresponding scores\n",
    "    return {\"boxes\": merged_boxes, \"scores\": torch.ones(len(merged_boxes), device=device)}\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\" Calculate Intersection-over-Union (IoU) between two bounding boxes. \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / union_area if union_area != 0 else 0\n",
    "\n",
    "\n",
    "def is_contained(box1, box2, threshold=0.9):\n",
    "    \"\"\" Checks if box2 is mostly contained within box1. \"\"\"\n",
    "    # Calculate the area of each box\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    if box1_area > 0 and (intersection_area / box1_area) > threshold:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot detections function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image, targets, detections, threshold=0.5):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Plot ground truth bounding boxes in green\n",
    "    for box in targets['boxes']:\n",
    "        x_min, y_min, x_max, y_max = box.cpu()  # Move to CPU if necessary\n",
    "        width, height = x_max - x_min, y_max - y_min\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Plot detected bounding boxes in red with confidence scores\n",
    "    for i, box in enumerate(detections['boxes']):\n",
    "        score = detections['scores'][i].cpu().item()  # Move to CPU and get the scalar value\n",
    "        \n",
    "        # Only plot if the score is above the threshold\n",
    "        if score >= threshold:\n",
    "            x_min, y_min, x_max, y_max = box.cpu()  # Move box coordinates to CPU\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_min, y_min - 5, f\"{score:.2f}\", color=\"red\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Detecções geradas pelo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/361...\n",
      "Running model on batch 1...\n",
      "Model run complete for batch 1\n",
      "Processed 2/721 images\n",
      "Processing batch 2/361...\n",
      "Running model on batch 2...\n",
      "Model run complete for batch 2\n",
      "Processed 4/721 images\n",
      "Processing batch 3/361...\n",
      "Running model on batch 3...\n",
      "Model run complete for batch 3\n",
      "Processed 6/721 images\n",
      "Processing batch 4/361...\n",
      "Running model on batch 4...\n",
      "Model run complete for batch 4\n",
      "Processed 8/721 images\n",
      "Processing batch 5/361...\n",
      "Running model on batch 5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run the model and time it\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning model on batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel run complete for batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Use custom merging for person detections\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensors)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torchvision\\models\\resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Run the model on the test images\n",
    "with torch.no_grad():\n",
    "    valid_images_tuned = []  # To keep track of images with a single person detection\n",
    "    total_images = len(data_loader.dataset)  # Get total number of images for progress tracking\n",
    "    processed_images = 0  # Counter to track progress\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(data_loader)}...\")\n",
    "        \n",
    "        # Load images to GPU and track progress\n",
    "        images = [image.to(device) for image in images]\n",
    "        processed_images += len(images)  # Update progress counter\n",
    "\n",
    "        # Run the model and time it\n",
    "        print(f\"Running model on batch {batch_idx + 1}...\")\n",
    "        outputs = model(images)\n",
    "        print(f\"Model run complete for batch {batch_idx + 1}\")\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            # Use custom merging for person detections\n",
    "            merged_output = custom_merge_boxes(output, score_threshold=0.25, containment_threshold=0.25)\n",
    "            \n",
    "            # Check if only one bounding box remains\n",
    "            if len(merged_output['boxes']) == 1:\n",
    "                image_id = targets[i]['image_id'].item()  # Assumes `image_id` is in `targets`\n",
    "                \n",
    "                # Track valid detections\n",
    "                valid_images_tuned.append((targets[i], merged_output, image_id))\n",
    "                # Experimentar remover esse tensor da lista, suspeito que ele esteja ocupando muita memória\n",
    "                # valid_images_tuned.append((images[i], targets[i], merged_output, image_id))\n",
    "        \n",
    "        # Print progress after each batch\n",
    "        print(f\"Processed {processed_images}/{total_images} images\")\n",
    "\n",
    "        # Optional: Stop after a few batches for testing\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Salvar detecções originais\n",
    "Salvei em um arquivo .json.\n",
    "\n",
    "**Importante não salvar o tensor image image o arquivo fica extremamente grande.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert tensors to lists for JSON compatibility\n",
    "valid_images_detection_results = [\n",
    "    {\n",
    "        \"target\": {k: (v.cpu().numpy().tolist() if isinstance(v, torch.Tensor) else v) for k, v in target.items()},\n",
    "        \"merged_output\": {\n",
    "            \"boxes\": merged_output[\"boxes\"].cpu().numpy().tolist(),\n",
    "            \"scores\": merged_output[\"scores\"].cpu().numpy().tolist() if \"scores\" in merged_output else []\n",
    "        },\n",
    "        \"image_id\": image_id\n",
    "    }\n",
    "    for target, merged_output, image_id in valid_images_tuned\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"deteccoes_originais.json\", \"w\") as f:\n",
    "    json.dump(valid_images_detection_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Adicionar padding nessas detecções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando detecções originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para os dados de treino\n",
    "with open(\"deteccoes_originais.json\", \"r\") as f:\n",
    "    valid_images_detections = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disso, acho que só preciso extrair:\n",
    "- Bounding box gerada\n",
    "- Width\n",
    "- Heigth\n",
    "- Image ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 - Função para extrair isso de cada uma entrada válida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_info(entrada_valida):\n",
    "    bbox_gerada = entrada_valida['merged_output']['boxes'][0]\n",
    "    bbox_gerada = bbox_gerada.cpu().numpy().tolist() if isinstance(bbox_gerada, torch.Tensor) else bbox_gerada # Checar se é um tensor\n",
    "    width, height = entrada_valida['target']['width'], entrada_valida['target']['height']\n",
    "    image_id = entrada_valida['image_id']\n",
    "    \n",
    "    return bbox_gerada, width, height, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.26876068115234, 191.50048828125, 157.19580078125, 337.08837890625] 640 480 393226\n"
     ]
    }
   ],
   "source": [
    "bbox, width, height, image_id = extrair_info(valid_images_detections[0])\n",
    "print(bbox, width, height, image_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recaptulando**\n",
    "\n",
    "- Agora tenho as detecções válidas em um arquivo .json.\n",
    "- Vou mostrar o número de detecções válidas/inválidas.\n",
    "- Depois adicionar o padding a essas detecções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecções válidas/totais: 507/721\n"
     ]
    }
   ],
   "source": [
    "print(f\"Detecções válidas/totais: {len(valid_images_detections)}/{len(data_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função Add padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percentage_padding_to_box(box, padding_percentage, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Adds padding to a bounding box as a percentage of its dimensions, keeping it within image bounds.\n",
    "    \n",
    "    Args:\n",
    "        box: Bounding box coordinates as [x_min, y_min, x_max, y_max].\n",
    "        padding_percentage: Padding as a percentage of the box dimensions (e.g., 15 for 15%).\n",
    "        image_width: Width of the image.\n",
    "        image_height: Height of the image.\n",
    "    \n",
    "    Returns:\n",
    "        Padded bounding box coordinates.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    \n",
    "    # Calculate the width and height of the bounding box\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    \n",
    "    # Calculate padding based on the percentage\n",
    "    x_padding = box_width * (padding_percentage / 100)\n",
    "    y_padding = box_height * (padding_percentage / 100)\n",
    "    \n",
    "    # Apply padding and ensure the box stays within image bounds\n",
    "    x_min = max(0, x_min - x_padding)\n",
    "    y_min = max(0, y_min - y_padding)\n",
    "    x_max = min(image_width, x_max + x_padding)\n",
    "    y_max = min(image_height, y_max + y_padding)\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image_path, original_bbox, new_bbox):\n",
    "    \"\"\"\n",
    "    Plots an image with the original and new (padded) bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Full path to the image.\n",
    "        original_bbox (list): List with the original bounding box [x_min, y_min, x_max, y_max].\n",
    "        new_bbox (list): List with the new bounding box (e.g., padded) [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image)  # Convert to numpy array for plotting\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    # Plot original bounding box in green\n",
    "    x_min, y_min, x_max, y_max = original_bbox\n",
    "    width, height = x_max - x_min, y_max - y_min\n",
    "    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='green', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Plot new bounding box in red\n",
    "    x_min, y_min, x_max, y_max = new_bbox\n",
    "    width, height = x_max - x_min, y_max - y_min\n",
    "    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 - Aplicar padding nas detecções originais\n",
    "- No final vamos ter um objeto que armazena:\n",
    "  - ID usada\n",
    "  - Bounding box de corte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detecoes_transformadas_com_ids = []\n",
    "padding_percentage = 10\n",
    "\n",
    "for entrada in valid_images_detections:\n",
    "    bbox, width, height, image_id = extrair_info(entrada)\n",
    "    new_bbox = add_percentage_padding_to_box(bbox, padding_percentage, width, height)\n",
    "    detecoes_transformadas_com_ids.append((image_id, new_bbox))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 - Salvar em json os resultados finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deteccoes_finais.json\", \"w\") as f:\n",
    "    json.dump(detecoes_transformadas_com_ids, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
