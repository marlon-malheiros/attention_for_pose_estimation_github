{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar se rodar em alguma cloud\n",
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca padrão\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from importlib import reload  # (Descomente se necessário)\n",
    "import random\n",
    "\n",
    "# Bibliotecas de terceiros\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# PyTorch e torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# (Descomente se necessário)\n",
    "sys.path.append(r'C:\\Users\\Marlon\\Nextcloud2\\Code_HPE_ResNet\\_arquivos_organizados')\n",
    "import minhas_funcoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Carregar detecções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_deteccoes = r'C:\\Users\\Marlon\\Nextcloud2\\Code_HPE_ResNet\\_arquivos_organizados\\detector_treinado\\deteccoes_finais.json'\n",
    "with open(path_deteccoes, \"r\") as f:\n",
    "    detecoes_transformadas_com_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Filtrar detecções válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Valid/Invalid samples: 15562/106\n",
      "15562\n"
     ]
    }
   ],
   "source": [
    "# Teste local com subset pequeno\n",
    "# ann_file = r'c:\\Users\\Marlon\\Downloads\\annotations\\annotations\\person_keypoints_val2017.json'\n",
    "# path_imgs = r'C:\\Users\\Marlon\\Downloads\\COCO_Dataset\\val2017'\n",
    "\n",
    "# Kaggle\n",
    "# ann_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json'\n",
    "# path_imgs = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "\n",
    "# Treinamento local\n",
    "ann_file = r\"c:\\Users\\Marlon\\Downloads\\annotations\\annotations\\person_keypoints_train2017.json\"\n",
    "path_imgs = r\"C:\\Users\\Marlon\\Downloads\\train_images\\train2017\"\n",
    "\n",
    "annotations_file = ann_file\n",
    "\n",
    "img_ids_bboxes = detecoes_transformadas_com_ids\n",
    "valid_samples, invalid_samples = minhas_funcoes.filter_images_by_keypoints(img_ids_bboxes, annotations_file)\n",
    "print(f\"Valid/Invalid samples: {len(valid_samples)}/{len(invalid_samples)}\")\n",
    "print(len(valid_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOPoseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, valid_samples, target_size=(256, 256), heatmap_size=(64, 64), transform=None):\n",
    "        \"\"\"\n",
    "        Custom COCO pose dataset for keypoint heatmap generation.\n",
    "        \n",
    "        Args:\n",
    "            image_dir (str): Directory containing the images.\n",
    "            annotations_file (str): Path to the COCO annotations JSON file.\n",
    "            valid_samples (list): List of tuples (image_id, bbox) for images with valid bboxes.\n",
    "            target_size (tuple): Desired image size after resizing and padding (h, w).\n",
    "            heatmap_size (tuple): Desired heatmap size for each keypoint (h, w).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotations_file)\n",
    "        self.valid_samples = valid_samples\n",
    "        self.target_size = target_size  # (h, w)\n",
    "        self.heatmap_size = heatmap_size  # (h, w)\n",
    "        self.keypoint_names = [\n",
    "            \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\", \"Left Shoulder\",\n",
    "            \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\", \"Left Wrist\", \"Right Wrist\",\n",
    "            \"Left Hip\", \"Right Hip\", \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\"\n",
    "        ]\n",
    "        # Change here to load the transforms\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def create_gaussian_heatmap(self, height, width, center_x, center_y, sigma=2):\n",
    "        \"\"\"Generates a 2D Gaussian heatmap centered at (center_x, center_y).\"\"\"\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        d2 = (x_grid - center_x) ** 2 + (y_grid - center_y) ** 2\n",
    "        gaussian = np.exp(-d2 / (2 * sigma ** 2))\n",
    "        return gaussian\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get the bounding box and image_id from valid_samples\n",
    "        image_id, bbox = self.valid_samples[idx]\n",
    "\n",
    "        # 2. Load image and retrieve annotation\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_name = image_info['file_name']\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_width, image_height = image.size  # (w, h)\n",
    "\n",
    "        # Adjust the bounding box if it exceeds the image boundaries\n",
    "        x_min, y_min, width, height = bbox\n",
    "        x_max = min(image_width, x_min + width)\n",
    "        y_max = min(image_height, y_min + height)\n",
    "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "        width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "        # Crop the image using the adjusted bounding box\n",
    "        image_cropped = image.crop((x_min, y_min, x_min + width, y_min + height))\n",
    "\n",
    "        # Resize the cropped image to the target size\n",
    "        aspect_ratio = image_cropped.height / image_cropped.width\n",
    "        if aspect_ratio > 1:\n",
    "            new_height = self.target_size[0]\n",
    "            new_width = int(self.target_size[1] / aspect_ratio)\n",
    "        else:\n",
    "            new_width = self.target_size[1]\n",
    "            new_height = int(self.target_size[0] * aspect_ratio)\n",
    "\n",
    "        image_resized = image_cropped.resize((new_width, new_height), Image.LANCZOS)\n",
    "        delta_h, delta_w = self.target_size[0] - new_height, self.target_size[1] - new_width\n",
    "        padding = (delta_w // 2, delta_h // 2, delta_w - delta_w // 2, delta_h - delta_h // 2)\n",
    "        image_padded = ImageOps.expand(image_resized, padding, fill=(0, 0, 0))\n",
    "\n",
    "        # Load and adjust keypoints\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=[self.coco.getCatIds(catNms=['person'])[0]], iscrowd=False)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        keypoints = annotations[0]['keypoints']\n",
    "        keypoints = [(keypoints[i], keypoints[i + 1], keypoints[i + 2]) for i in range(0, len(keypoints), 3)]\n",
    "        adjusted_keypoints = [(x - x_min, y - y_min, v) for x, y, v in keypoints]\n",
    "        scale_y, scale_x = new_height / height, new_width / width\n",
    "        scaled_keypoints = [(x * scale_x, y * scale_y, v) for x, y, v in adjusted_keypoints]\n",
    "        adjusted_keypoints_padded = [(x + padding[0], y + padding[1], v) for x, y, v in scaled_keypoints]\n",
    "\n",
    "        # Generate heatmaps for each keypoint\n",
    "        scale_y_hm = self.heatmap_size[0] / self.target_size[0]\n",
    "        scale_x_hm = self.heatmap_size[1] / self.target_size[1]\n",
    "        heatmaps = []\n",
    "        for x, y, v in adjusted_keypoints_padded:\n",
    "            if v == 2:\n",
    "                x_hm, y_hm = int(x * scale_x_hm), int(y * scale_y_hm)\n",
    "                heatmap = self.create_gaussian_heatmap(self.heatmap_size[0], self.heatmap_size[1], x_hm, y_hm, sigma=2)\n",
    "            else:\n",
    "                # Also i think i will change this behaviour\n",
    "                # It can be better to create a heatmap with the peak at (0,0), just like in the dataset right?\n",
    "                heatmap = np.zeros(self.heatmap_size)\n",
    "                \n",
    "            heatmaps.append(torch.tensor(heatmap))\n",
    "\n",
    "        # Convert final image to tensor and return\n",
    "        image_tensor = torch.tensor(np.array(image_padded)).permute(2, 0, 1).float() / 255.0\n",
    "        # heatmaps_tensor = torch.stack(heatmaps)\n",
    "        heatmaps_tensor = torch.stack(heatmaps).float()  # Ensure float dtype for heatmaps\n",
    "\n",
    "        # If you have a transform for image+heatmaps:\n",
    "        if self.transform is not None:\n",
    "            # image_tensor should be a float tensor and heatmaps_tensor a float tensor before transform\n",
    "            # Make sure both are on CPU and float type\n",
    "            # image_tensor = image_tensor.to(torch.float)\n",
    "            # heatmaps_tensor = heatmaps_tensor.to(torch.float)\n",
    "            image_tensor, heatmaps_tensor = self.transform(image_tensor, heatmaps_tensor)\n",
    "\n",
    "        return image_tensor, heatmaps_tensor, adjusted_keypoints_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Classe para sincronizar augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPairTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image_tensor, heatmaps_tensor):\n",
    "        # Ensure image_tensor is float in range [0, 1]\n",
    "        assert image_tensor.dtype == torch.float, \"Image tensor must be of dtype torch.float\"\n",
    "        assert image_tensor.max() <= 1.0, \"Image tensor values must be in range [0, 1]\"\n",
    "\n",
    "        # Horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            image_tensor = TF.hflip(image_tensor)\n",
    "            heatmaps_tensor = torch.flip(heatmaps_tensor, dims=[2])  # Flip along width (W)\n",
    "\n",
    "        # Random rotation\n",
    "        angle = random.uniform(-15, 15)\n",
    "        image_tensor = TF.rotate(image_tensor, angle)  # Rotate image tensor\n",
    "        heatmaps_tensor = TF.rotate(heatmaps_tensor.unsqueeze(1), angle).squeeze(1)  # Rotate heatmaps\n",
    "\n",
    "        # Add more transformations as needed\n",
    "\n",
    "        return image_tensor, heatmaps_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Inicializar modelo de estimação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo backbone\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "model = models.resnet50(weights=weights)\n",
    "backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# Estimation head\n",
    "class PoseEstimationHead(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super(PoseEstimationHead, self).__init__()\n",
    "        \n",
    "        # Reduzir os channels de 2048 para 256\n",
    "        self.conv1 = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv_middle = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Upsampling\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) # 7x7 -> 14x14        \n",
    "        \n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) # 14x14 -> 28x28\n",
    "        \n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) # 28x28 -> 56x56\n",
    "        \n",
    "        # Final conv layer\n",
    "        self.final_conv = nn.Conv2d(256, num_keypoints, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 2048x7x7 -> 256x7x7\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.upsample1(x) # 256x7x7 -> 256x14x14\n",
    "        x = self.conv_middle(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 256x14x14 -> 256x28x28\n",
    "        x = self.conv_middle(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.upsample3(x) # 256x28x28 -> 256x56x56\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class PoseEstimationModel(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super(PoseEstimationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = PoseEstimationHead(num_keypoints)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = PoseEstimationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Move the Model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Setup data augment e data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.67s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "size_input = (224, 224)\n",
    "size_heatmap = (56, 56)\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = MyPairTransform()\n",
    "\n",
    "# Update the dataset class to apply transformations\n",
    "dataset = COCOPoseDataset(path_imgs, annotations_file, valid_samples, size_input, size_heatmap, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "val_split = 0.1\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(val_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assuming `model` is already defined; replace with your model initialization if not\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Loop de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Bacthes:  27%|██▋       | 238/876 [01:41<04:32,  2.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Bacthes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mCOCOPoseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     42\u001b[0m image_name \u001b[38;5;241m=\u001b[39m image_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, image_name)\n\u001b[1;32m---> 44\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m image_width, image_height \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize  \u001b[38;5;66;03m# (w, h)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Adjust the bounding box if it exceeds the image boundaries\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marlon\\miniforge3\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store losses for plotting later\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Initialize a list to store learning rates\n",
    "learning_rate_log = []\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "patience_counter = 0  # Initialize the patience counter\n",
    "\n",
    "# Minimum number of epochs\n",
    "min_epochs = 50\n",
    "max_epochs = 200\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Track epoch start time\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, heatmaps, _ in tqdm(train_loader, desc=\"Training Bacthes\"):\n",
    "        inputs, heatmaps = inputs.float().to(device), heatmaps.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \"\"\"\n",
    "        - Masking the validation loss for visible points\n",
    "        - Avaliar depois impacto da validation mask\n",
    "        - Acho que é melhor manter, se remover podemos acabar penalizando o modelo por não prever pontos que não são visíveis\n",
    "        \"\"\"\n",
    "        mask = (heatmaps.sum(dim=(2, 3)) > 0).float().unsqueeze(-1).unsqueeze(-1)  # Shape: [batch_size, num_keypoints, 1, 1]\n",
    "        visible_outputs = outputs * mask  # Apply mask to outputs\n",
    "        visible_heatmaps = heatmaps * mask  # Apply mask to ground truth heatmaps\n",
    "\n",
    "        # Compute masked loss\n",
    "        loss = criterion(visible_outputs, visible_heatmaps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Após o loop de treinamento, calcular a média da loss\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Training Loss: {epoch_train_loss:.8f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_heatmaps, _ in val_loader:\n",
    "            val_inputs, val_heatmaps = val_inputs.float().to(device), val_heatmaps.float().to(device)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "\n",
    "            val_mask = (val_heatmaps.sum(dim=(2, 3)) > 0).float().unsqueeze(-1).unsqueeze(-1)            \n",
    "            visible_val_outputs = val_outputs * val_mask\n",
    "            visible_val_heatmaps = val_heatmaps * val_mask\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss += criterion(visible_val_outputs, visible_val_heatmaps).item() * val_inputs.size(0)\n",
    "    \n",
    "    # Validation loss   \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {epoch_val_loss:.8f}\")\n",
    "    \n",
    "    # Epoch end time\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} took {epoch_duration:.2f} seconds.\")\n",
    "\n",
    "    # Record the learning rate after each epoch\n",
    "    current_lr = optimizer.param_groups[0]['lr']  # Assuming a single parameter group\n",
    "    learning_rate_log.append(current_lr)\n",
    "    print(f\"Epoch {epoch+1}, Learning Rate: {current_lr}\")\n",
    "    \n",
    "    # Check for Early Stopping and Save Best Model Checkpoint\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        save_path = f\"change_saved_file_name.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': epoch_train_loss,\n",
    "            'val_loss': epoch_val_loss\n",
    "        }, save_path)  # Save the best model checkpoint\n",
    "        print(\"Model improved. Saving the best model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience and epoch >= min_epochs:\n",
    "        print(f\"Min number epochs reached, with no improvement in validation loss. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    # If gets to max number of epochs\n",
    "    if epoch == max_epochs:\n",
    "        print(f\"Reached maximum number of epochs ({max_epochs}). Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "# Save the learning rate log to a file (optional)\n",
    "with open(\"learning_rate_log.txt\", \"w\") as f:\n",
    "    for lr in learning_rate_log:\n",
    "        f.write(f\"{lr}\\n\")\n",
    "\n",
    "with open(\"train_losses.txt\", \"w\") as f:\n",
    "    for value in train_losses:\n",
    "        f.write(f\"{value}\\n\")\n",
    "\n",
    "with open(\"val_losses.txt\", \"w\") as f:\n",
    "    for value in val_losses:\n",
    "        f.write(f\"{value}\\n\")\n",
    "\n",
    "print(f\"End training loop after {epoch + 1} epochs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
